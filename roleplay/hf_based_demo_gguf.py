# coding=utf-8
from src.logger import LoggerFactory
from src.prompt_concat import GetManualTestSamples, CreateTestDataset
from src.utils import decode_csv_to_json, load_json, save_to_json
from threading import Thread
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    GenerationConfig,
    TextIteratorStreamer,
)
from queue import Queue
from llama_cpp import Llama
from typing import List

import gradio as gr
import logging
import os
import shutil
import torch
import warnings

logger = LoggerFactory.create_logger(name="test", level=logging.INFO)
warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')

config_data = load_json("config/config.json")
gguf_model_path = config_data["gguf_model_local_path"]
character_path = "./character"
llm = Llama(model_path = gguf_model_path, n_gpu_layers=-1, verbose=True, n_ctx=0)
# model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16,trust_remote_code=True)


def generate_with_question(question, role_name, max_seq_len=2048):
    question_in = "\n".join(["\n".join(pair) for pair in question])

    g = GetManualTestSamples(
        role_name=role_name,
        role_data_path=f"./character/{role_name}.json",
        save_samples_dir="./character",
        save_samples_path=role_name + "_rag.json",
        prompt_path="./prompt/dataset_character.txt",
        max_seq_len=max_seq_len
    )
    g.get_qa_samples_by_query(
        questions_query=question_in,
        keep_retrieve_results_flag=True
    )


def create_datasets(role_name):
    testset = []
    role_samples_path = os.path.join("./character", role_name + "_rag.json")

    c = CreateTestDataset(role_name=role_name,
                          role_samples_path=role_samples_path,
                          role_data_path=role_samples_path,
                          prompt_path="./prompt/dataset_character.txt"
                          )
    res = c.load_samples()
    testset.extend(res)
    save_to_json(testset, f"./character/{role_name}_æµ‹è¯•é—®é¢˜.json")


def hf_gen(dialog: List, role_name, top_k, top_p, temperature, repetition_penalty, max_dec_len, max_seq_len):
    generate_with_question(dialog, role_name, max_seq_len)
    create_datasets(role_name)

    json_data = load_json(f"{character_path}/{role_name}_æµ‹è¯•é—®é¢˜.json")[0]
    text = json_data["input_text"]
    messages = [{"role": "user", "content": text}]

    generation_kwargs = dict(
        messages = messages,
        top_k=int(top_k),
        top_p=float(top_p),
        temperature=float(temperature),
        repeat_penalty=float(repetition_penalty),
        max_tokens=int(max_dec_len),
        stream = True
    )

    # åˆ›å»ºä¸€ä¸ªQueueæ¥ä¼ é€’ç”Ÿæˆçš„æ–‡æœ¬
    queue = Queue()

    # å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¿è¡Œç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å°†è¾“å‡ºæ”¾å…¥Queue
    def run_generation():
        for new_text in llm.create_chat_completion(**generation_kwargs):
            if 'content' in new_text['choices'][0]['delta']:
                if new_text['choices'][0]['delta']['content'] == '':
                    continue
                queue.put(new_text['choices'][0]['delta']['content'])

        queue.put(None)  # ä½¿ç”¨ None ä½œä¸ºç»“æŸä¿¡å·

    # å¯åŠ¨çº¿ç¨‹è¿è¡Œç”Ÿæˆå‡½æ•°
    thread = Thread(target=run_generation)
    thread.start()

    # ä»Queueä¸­è¯»å–ç”Ÿæˆçš„æ–‡æœ¬å¹¶é€æ­¥è¿”å›
    answer = ""
    while True:
        chunk = queue.get()
        if chunk is None:
            break
        answer += chunk
        yield answer  # è¿”å›æ–°ç”Ÿæˆçš„æ–‡æœ¬


def generate(chat_history: List, query, role_name, user_name, top_k, top_p, temperature, repetition_penalty, max_dec_len, max_seq_len):
    """generate after hitting "submit" button

    Args:
        chat_history (List): [[q_1, a_1], [q_2, a_2], ..., [q_n, a_n]]. list that stores all QA records
        query (str): query of current round
        top_p (float): only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.
        temperature (float): strictly positive float value used to modulate the logits distribution.
        max_dec_len (int): The maximum numbers of tokens to generate.

    Yields:
        List: [[q_1, a_1], [q_2, a_2], ..., [q_n, a_n], [q_n+1, a_n+1]]. chat_history + QA of current round.
    """
    assert query != "", "Input must not be empty!!!"
    # apply chat template
    chat_history.append([f"{user_name}:{query}", ""])
    for answer in hf_gen(chat_history, role_name, top_k, top_p, temperature, repetition_penalty, max_dec_len, max_seq_len):
        chat_history[-1][1] = role_name + ":" + answer
        yield gr.update(value=""), chat_history


def regenerate(chat_history: List,role_name,role_desc, top_k, top_p, temperature, repetition_penalty, max_dec_len, max_seq_len):
    """re-generate the answer of last round's query

    Args:
        chat_history (List): [[q_1, a_1], [q_2, a_2], ..., [q_n, a_n]]. list that stores all QA records
        top_p (float): only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.
        temperature (float): strictly positive float value used to modulate the logits distribution.
        max_dec_len (int): The maximum numbers of tokens to generate.

    Yields:
        List: [[q_1, a_1], [q_2, a_2], ..., [q_n, a_n]]. chat_history
    """
    assert len(chat_history) >= 1, "History is empty. Nothing to regenerate!!"
    if len(chat_history[-1]) > 1:
        chat_history[-1][1] = ""
    # apply chat template
    for answer in hf_gen(chat_history, role_name, top_k, top_p, temperature, repetition_penalty, max_dec_len, max_seq_len):
        chat_history[-1][1] = role_name + ":" + answer
        yield gr.update(value=""), chat_history


def clear_history():
    """clear all chat history

    Returns:
        List: empty chat history
    """
    torch.cuda.empty_cache()
    return []

def save_history(chat_history: List, role_name):
    """save chat history to local file

    Args:
        chat_history (List): [[q_1, a_1], [q_2, a_2], ..., [q_n, a_n]]. list that stores all QA records
        role_name (str): role name

    Returns:
        List: chat history
    """
    role_data_path = os.path.join("./character", role_name + ".json")
    role_data = load_json(role_data_path)

    for chat in chat_history:
        role_data.append(role_data[0].copy())
        role_data[-1]["dialog"] = chat
    save_to_json(role_data, role_data_path)
    gr.Info(f"{role_name}å¯¹è¯ä¿å­˜æˆåŠŸ")
    

def generate_file(file_obj, role_info):
    role_name = os.path.basename(file_obj).split(".")[0]
    file_name = f"{role_name}.csv"
    new_path = os.path.join(character_path, file_name)
    shutil.copy(file_obj, new_path)
    decode_csv_to_json(os.path.join(character_path, role_name + ".csv"), role_name, role_info,
                       os.path.join(character_path, role_name + ".json"))
    gr.Info(f"{role_name}ç”ŸæˆæˆåŠŸ")


def main():
    # launch gradio demo
    with gr.Blocks(theme="soft") as demo:
        gr.Markdown("""# Index-1.9B è§’è‰²æ‰®æ¼”æ¼”ç¤º""")

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("""## ğŸ‘¼ è§’è‰²ç”Ÿæˆ""")
                file_input = gr.File(label="ä¸Šä¼ è§’è‰²å¯¹è¯è¯­æ–™(è§’è‰²åç§°.csv)")
                role_description = gr.Textbox(label="è§’è‰²æè¿°", placeholder="è¯·è¾“å…¥è§’è‰²æè¿°ã€‚", lines=2)
                upload_button = gr.Button("ç”Ÿæˆè§’è‰²!")
                upload_button.click(generate_file, inputs=[file_input, role_description])

                gr.Markdown("""## ğŸ”§ æ¨¡å‹å‚æ•°""")
                top_k = gr.Slider(0, 10, value=5, step=1, label="å•æ­¥é‡‡ç”¨Tokençš„æ•°é‡")
                top_p = gr.Slider(0.0, 1.0, value=0.8, step=0.1, label="å•æ­¥ç´¯è®¡é‡‡ç”¨é˜ˆå€¼")
                temperature = gr.Slider(0.1, 2.0, value=0.85, step=0.1, label="ç”Ÿæˆæ–‡æœ¬éšæœºæ€§ï¼ˆæ¸©åº¦ï¼‰")
                repetition_penalty = gr.Slider(0.1, 2.0, value=1.0, step=0.1, label="é‡å¤æƒ©ç½šç³»æ•°")
                max_dec_len = gr.Slider(1, 4096, value=512, step=1, label="æœ€å¤§è¾“å‡ºé•¿åº¦")
                max_seq_len = gr.Slider(1, 4096, value=2048, step=1, label="æœ€å¤§è¾“å…¥é•¿åº¦")

            with gr.Column(scale=10):
                gr.Markdown("""## ğŸ˜€ è§’è‰²æ‰®æ¼”""")
                chatbot = gr.Chatbot(bubble_full_width=False, height=600, label='Index-1.9B')
                with gr.Row():
                    role_name = gr.Textbox(label="èŠå¤©è§’è‰²", placeholder="è¾“å…¥èŠå¤©è§’è‰²åç§°ã€‚", lines=2, value="ä¸‰ä¸‰")
                    user_name = gr.Textbox(label="ä½ çš„åå­—", placeholder="è¾“å…¥ä½ çš„åå­—ã€‚", lines=2, value="ä¸»äºº")
                user_input = gr.Textbox(label="ç”¨æˆ·", placeholder="è¾“å…¥ä½ çš„èŠå¤©ã€‚", lines=2)
                with gr.Row():
                    submit = gr.Button("ğŸš€ æäº¤å¯¹è¯")
                    clear = gr.Button("ğŸ§¹ æ¸…ç©ºå¯¹è¯")
                    regen = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆ")
                    save = gr.Button("ğŸ’¾ ä¿å­˜å¯¹è¯")

        submit.click(generate, inputs=[chatbot, user_input, role_name, user_name, top_k, top_p, temperature,
                                       repetition_penalty, max_dec_len, max_seq_len],
                     outputs=[user_input, chatbot])
        regen.click(regenerate,
                    inputs=[chatbot, role_name, user_name, top_k, top_p, temperature, repetition_penalty,
                            max_dec_len, max_seq_len],
                    outputs=[user_input, chatbot])
        save.click(save_history, inputs=[chatbot, role_name], outputs=[])
        clear.click(clear_history, inputs=[], outputs=[chatbot])

    demo.queue()
    demo.launch(server_name="0.0.0.0",
                server_port=8002,
                show_error=True,
                share=False)


if __name__ == "__main__":
    main()
